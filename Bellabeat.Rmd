---
title: "Case Study: Bellabeat"
author: Petel Sener
date: 28-12-2022
output: html_document
---


#### Introduction

This is a case study from [Google's Data Analytics coursera](https://www.coursera.org/professional-certificates/google-data-analytics) course. The original case study instructions can be found [here](https://d18ky98rnyall9.cloudfront.net/QWpX1RwgS1CqV9UcILtQQA_76f613fdf2844940b0b31f8f0bb1b83f_Case-Study-2-_-How-can-a-wellness-technology-company-play-it-smart.pdf?Expires=1672617600&Signature=W4d6WueYc~LJqaTqnev9NME9cMuo26xmL6freIztG17ioG7rcs1FdL8Dyesn2kqcKfq8xJ9P5XsMDow7tkcfXNhV~7iryk3ThOQYMQ0JLiT19ou963DHtbv9Ly6yb65Q~jlQP8gTAXGuGO2UDW-099QxO0TbmrrrlIPDgFMmxEk_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A). The case study roadmap includes 6 phases: Ask, Prepare, Process, Analyze, Share, and Act. 

For this case study, I chose to use SQL (MySQL), R, and Tableau.

#### 1) Ask

The "Ask" phase is for identifying the problem, the business task, the stakeholders, and the context of the project.


*What is the problem you are trying to solve?*

The goal is to determine the current trends in smart device usage and translate that knowledge to make marketing strategy recommendations for Bellabeat's smart device products. Understanding the trends and behavior of the target consumers can help inform marketing strategy decisions, and overall help increase sales and customer satisfaction.


*Who are the key stakeholders?*

The stakeholders for this case study are the Bellabeat team including the CCO, Urška Srše, the executive team member Sando Mur, and the Bellabeat marketing analytics team.


*Identify the business task*

The business task is to improve marketing strategy for Bellabeat smart devices by studying and discovering current trends in consumer use of non-Bellabeat smart devices.


#### 2) Prepare

The "Prepare" phase is for examining the data. It is important to investigate the source, format, organization, credibility, and consistency of the data.


*Where is the data stored? How is the data organized?*

The data is sourced from a [Kaggle page](https://www.kaggle.com/datasets/arashnic/fitbit). According to the Kaggle page, the data is "generated by respondents to a distributed survey via Amazon Mechanical Turk between 03.12.2016-05.12.2016. Thirty eligible Fitbit users consented to the submission of personal tracker data, including minute-level output for physical activity, heart rate, and sleep monitoring."

The data provided consists of 18 CSV files. Overall, the data includes information about each participant's time spent sleeping, time spent in bed, calories burned, steps walked, distance traveled, weight changed, and minutes exercised. Some of these categories are broken down by day, by hour and by minute. The detailed description of each file is below.

| Filename      | Description |
| ----------- | ----------- |
| dailyActivity_merged.csv      | Steps traveled, calories burned, and minutes spent in activity for each participant for each day       |
| dailyCalories_merged.csv   | Calories burned by each participant for each day        |
| dailyIntensities_merged.csv | Minutes spent in different levels of activity and distance traveled for each participant for each day |
| dailySteps_merged.csv | Total steps taken for each participant for each day |
| heartrate_seconds_merged.csv | Heart rate for participant for various days and various times in the day |
| hourlyCalories_merged.csv | Calories burned per participant per hour per day |
| hourlyIntensities_merged.csv | Activity intensity per participant per hour per day |
| hourlySteps_merged.csv | Steps traveled per participant per hour per day |
| minuteCaloriesNarrow_merged.csv | Calories burned per participant per minute  |
| minuteCaloriesWide_merged.csv | Calories burned per participant per minute in a wide format |
| minutesIntensitiesNarrow_merged.csv | Activity intensity per participant per minute |
| minutesIntensitiesWide_merged.csv |Activity intensity per participant per minute in a wide format |
| minutesMETsNarrow_merged.csv |  No sufficient explanation of the data |
| minutesSleep_merged.csv | Minutes spent sleeping and in bed per participant per minute per day |
| minutesStepsNarrow_merged.csv | Steps traveled per participant per minute |
| minutesStepsWide_merged.csv | Steps traveled per participant per minute in a wide format |
| sleepDay_merged.csv | Minutes spent sleeping and in bed per participant per day |
| weightLogInfo_merged.csb | Participation weight measured in kg and pounds, BMI, and fat content for various days |


*How does it help you answer your question?*

The data provides information about the habits and behavior of consumers in regards to daily activity, weight change and sleeping patterns. Not all of the CSV files are useful in addressing the business task and problem. I initially tried to open the CSV files on MySQL to get an idea of the data within each file. Unfortunately, I ran into issues opening the CSV files on MySQL because the data did not have consistent data types within each column. I switched to using GoogleSheets to get a preview of the files. After importing and reviewing all the CSV files on GoogleSheets, I decided to use a subset of the CSV files including:

- "dailyActivity_merged.csv"

- "hourlyCalories_merged.csv"

- "hourlySteps_merged.csv"

- "sleepDay_merged.csv"

- "weightLogINfo_merged.csv" 

Other files in the data set either do not include useful information for the problem at hand or have columns with unclear column names.


*What is the data credibility and integrity?*

The data source "Amazon Mechanical Turk" is credible however, it is uncertain if the copy of the data that is available on the Kaggle page accurately captured the original data set. In addition, the data has multiple limitations including:

- The data sample size is 30 participants. This is relatively small.
 
- The data includes information on only one smart device (Fitbit). Examining one smart device may not be enough discover  trends for all smart devices.
 
- The time range of the data is approximately 2 months which is not a very long time.
 
- The data is from 2016 which is not current in the year of 2022.



As part of the Prepare phase, I formatted and cleaned the data. For each of the selected CSV files, I used GoogleSheets to check for:

- Duplicate rows. There were 3 duplicate rows in "sleepDay_merged.csv"

- Extra white spaces inside cells.


Afterwards, I formatted the data to have consistent data type in each column.

- Formatted whole numbers as integers

- Formatted decimal numbers to have 2 decimal precision

- Formatted dates to be in YYY-MM-DD HH:MM:SS 


#### 3) Process

The "Process" phase involves transforming the data before the analysis. Transforming can involve sorting, filtering, adding or removing columns, joining tables, etc.


*What tools are you choosing and why?*

I chose to use SQL (MySQL) because it can handle large data sets very well. I find it easier to transform and clean the data in SQL.


##### 3.1) Cleaning the data

Note: I used MySQL to clean, format and transform the data. I'm transferring my SQL code to this document to compile all of my work into one document. 

First I created a SQL database and added the contents of the CSV files into the database as tables.
```{r setup}
# Install and load the necessary packages

# install.packages("DBI") # main DB interface
# install.packages("RSQLite") # To connect to SQLite DB
# install.packages("tidyverse") # for dplyr, readr etc.
# install.packages("dbplyr") # dplyr backend for DBs

library(DBI)
library(RSQLite)
library(tidyverse)
library(dbplyr)
library(ggplot2)
```


As explained in Prepare phase, I decided to use 5 of the 18 CSV files. I'm focusing on the hourly and daily data.
```{r load csv files}
# Load the necessary csv files from the provided data set.
dailyActivity_merged <- read_csv("/Users/peteksener/Bellabeat - Case Study/datasets_post_formatting/dailyActivity_merged.csv")
hourlyCalories_merged <- read_csv("/Users/peteksener/Bellabeat - Case Study/datasets_post_formatting/hourlyCalories_merged.csv")
hourlySteps_merged <- read_csv("/Users/peteksener/Bellabeat - Case Study/datasets_post_formatting/hourlySteps_merged.csv")
sleepDay_merged <- read_csv("/Users/peteksener/Bellabeat - Case Study/datasets_post_formatting/sleepDay_merged.csv")
weightLogInfo_merged <- read_csv("/Users/peteksener/Bellabeat - Case Study/datasets_post_formatting/weightLogInfo_merged.csv")

# Had to convert the dates before adding them to database or else SQLlite converts dates to string of date values
dailyActivity_merged$ActivityDate<- as.character(dailyActivity_merged$ActivityDate)
hourlyCalories_merged$ActivityHour <- as.character(hourlyCalories_merged$ActivityHour)
hourlySteps_merged$ActivityHour <- as.character(hourlySteps_merged$ActivityHour)
sleepDay_merged$SleepDay <- as.character(sleepDay_merged$SleepDay)
weightLogInfo_merged$Date <- as.character(weightLogInfo_merged$Date)
```


```{r create database for SQL}
# Create a SQLite database and open a connection
con <- DBI::dbConnect(RSQLite::SQLite(), dbname = "bellabeat.sqlite")

# Write the dataframes into database as tables. I renamed the dataframe so the table names are more consistent
dbWriteTable(con,
             "daily_activity", # table is named daily_activity
             dailyActivity_merged, # dataframe being written
             overwrite = TRUE)

dbWriteTable(con, 
             "hourly_calories", 
             hourlyCalories_merged,
             overwrite = TRUE)
dbWriteTable(con, 
             "hourly_steps", 
             hourlySteps_merged, 
             overwrite = TRUE)
dbWriteTable(con, 
             "daily_sleep", 
             sleepDay_merged, 
             overwrite = TRUE)
dbWriteTable(con, 
             "weight_log", 
             weightLogInfo_merged, 
             overwrite = TRUE)

# close the connection
dbDisconnect(con)


# Start a connection to the newly created database 
bellabeat_con <- dbConnect(RSQLite::SQLite(), "bellabeat.sqlite")

# List tables in database to check they were properly created
dbListTables(bellabeat_con)
```

After creating the database, I checked that all the tables were properly created.
```{sql check tables, connection=bellabeat_con}
-- Check all the rows were imported from CSV files
SELECT COUNT(*) FROM daily_activity;
SELECT COUNT(*) FROM hourly_steps;
SELECT COUNT(*) FROM hourly_calories;
SELECT COUNT(*) FROM daily_sleep;
SELECT COUNT(*) FROM weight_log;
```

Now that all the raw data is in the database, we can start cleaning and transforming the tables using SQL. First, I looked through the data to check for consistency. I checked the unique participant ids, and the dates within each table. 

Looking through the disinct ids on each table, I discovered that:

* There are 33 unique participants and not 30 as the Kaggle page stated. This raises questions about the integrity of the data. I'm going to assume there were actually 33 participants.

* The daily_sleep and weight_log files do not contain data for all of the participants.

```{sql check ids, connection=bellabeat_con}
-- Compare the unique id in each table to check if all tables contain the same ids
SELECT 
	DISTINCT da.id AS daily_activity_ids, 
	ds.id AS daily_sleep_ids, 
	hc.id AS hourly_calories_ids,
	hs.id AS hourly_steps_ids,
	wl.id AS weight_log_ids
FROM daily_activity da 
LEFT JOIN (SELECT DISTINCT id FROM daily_sleep) ds 
	ON ds.id = da.id 
LEFT JOIN (SELECT DISTINCT id FROM hourly_calories) hc 
	ON hc.id = da.id
LEFT JOIN (SELECT DISTINCT id  FROM hourly_steps) hs 
	ON hs.id = da.id
LEFT JOIN (SELECT DISTINCT id FROM weight_log) wl 
	ON wl.id = da.id;
	
	-- Check the number of unique ids in each table
SELECT COUNT(DISTINCT id) FROM daily_activity;
SELECT COUNT(DISTINCT id) FROM hourly_steps;
SELECT COUNT(DISTINCT id) FROM hourly_calories;
SELECT COUNT(DISTINCT id) FROM daily_sleep;
SELECT COUNT(DISTINCT id) FROM weight_log;


```

After checking the ids, I proceeded with checking the dates in the tables.

For hourly_calories and hourly_steps files I decided to split the date into two separate columns: a date column and a time column.
Note: In MySQL I used VIEW instead of creating a new table.
```{sql format hourly_calories, connection=bellabeat_con}
-- Create new hourly view where the datetime component is split into date and time columns
CREATE VIEW IF NOT EXISTS hourly_calories_formatted
AS 
  SELECT 
		id,
    date(ActivityHour) AS date, 
    time(ActivityHour) AS time, 
    calories 
	FROM hourly_calories
	;
```
```{sql hourly_calories_formatted view, connection=bellabeat_con}
	SELECT * FROM hourly_calories_formatted;
```




```{sql format hourly_steps, connection=bellabeat_con}
CREATE VIEW IF NOT EXISTS hourly_steps_formatted
AS 
  SELECT 
		id,
    date(ActivityHour) AS date, 
    time(ActivityHour) AS time, 
    StepTotal 
	FROM hourly_steps
	;

```
```{sql hourly_steps_formatted view, connection=bellabeat_con}
SELECT * FROM hourly_steps_formatted;
```

After splitting the date column, I looked at the number of days each participant had data for. Here I discovered that the data is not complete. There are multiple participants that are missing several days of calorie, steps, activity and sleep data. Based on this information, I decided to remove participant "4057192912" because they have at most 4 days of data collection across all tables.
```{sql check number of data collection dates for each participant, connection=bellabeat_con}
-- Join all tables to get a overall view of each participant's number of data points
SELECT id, hc.num_days_calories_collected, hs.num_days_steps_collected, ds.num_days_sleep_collected, da.num_days_activity_collected
FROM (
	SELECT id, COUNT(DISTINCT date) AS num_days_calories_collected 
    FROM hourly_calories_formatted 
    GROUP BY (id)
    ) hc
LEFT JOIN (
	SELECT id, COUNT(DISTINCT date) AS num_days_steps_collected 
    FROM hourly_steps_formatted 
    GROUP BY (id)
) hs USING(id)
LEFT JOIN (
	SELECT id, COUNT(DISTINCT sleepday) AS num_days_sleep_collected 
    FROM daily_sleep
    GROUP BY (id)
) ds USING(id)
LEFT JOIN (
	SELECT id, COUNT(DISTINCT activitydate) AS num_days_activity_collected 
    FROM daily_activity
    GROUP BY (id)
) da USING(id)
;

```

After discovering that not all participants have the same number of data collection days, I decided to check the hourly data tables as well. I wanted to check if there were actually 24 hour data points for each day. Here I discovered that the hourly data does not always cover the full 24 hours for each participant for each day. After looking at the data, I decided to remove date "2016-05-12" from the tables. Multiple participants have less than 24 observations on "2016-05-12".
```{sql checking number of hours for each participant, connection=bellabeat_con}
-- Checking the number of hourly data points for each day. To make the viewing easier I only kept the days where there was NOT 24 rows of data
SELECT id, date, hc.num_hours_calories_collected, hs.num_hours_steps_collected
FROM (
	SELECT id, date , COUNT(DISTINCT time) AS num_hours_calories_collected
FROM hourly_calories_formatted 
GROUP BY id, date
HAVING COUNT(DISTINCT time) < 24
    ) hc
JOIN (
	SELECT id, date , COUNT(DISTINCT time) AS num_hours_steps_collected
FROM hourly_steps_formatted 
GROUP BY id, date
HAVING COUNT(DISTINCT time) < 24
) hs USING(id , date);
```



Based on my discoveries, I removed participant "4057192912" and date "2016-05-12" from the data tables. I also reformatted and transformed the tables by joining relevant tables together. 

I formatted daily_activity table.
```{sql format daily_activity, connection=bellabeat_con}
-- Combine daily activity and daily sleep, filter daily activity to categories
-- REMOVE 2016-05-12 AND 4057192912 
CREATE VIEW  IF NOT EXISTS daily_activity_formatted AS 
SELECT 
	id, 
	date(activityDate) AS date, 
	totalSteps AS total_steps, 
    calories AS total_calories,
	veryactiveminutes AS very_active_mins, 
	fairlyactiveminutes AS fairly_active_mins,
    lightlyactiveminutes AS lightly_active_mins,
    sedentaryminutes AS sedentary_mins
FROM daily_activity
WHERE id <> "4057192912" AND date(activityDate) <> "2016-05-12"
;
```
```{sql daily_activity_formatted, connection=bellabeat_con}
SELECT * FROM daily_activity_formatted;
```


I formatted the daily_sleep table.
```{sql format daily_sleep ,connection=bellabeat_con}
CREATE VIEW  IF NOT EXISTS daily_sleep_formatted AS 
SELECT 
	id, 
	date(sleepday) AS date, 
	totalminutesasleep AS total_mins_asleep, 
    totaltimeinbed AS total_mins_in_bed
FROM daily_sleep
WHERE id <> "4057192912" AND CAST(sleepday as date) <> "2016-05-12"
;
```
```{sql daily_sleep_formatted, connection=bellabeat_con}
SELECT * FROM daily_sleep_formatted;

```

I combined the formatted daily activity and sleep tables
```{sql joined fromatted daily_activity and daily_sleep, connection=bellabeat_con}

CREATE TABLE  IF NOT EXISTS daily_activity_and_sleep AS 
SELECT 
	daf.id, 
    daf.date, 
    daf.total_steps, 
    daf.total_calories, 
    daf.very_active_mins, 
    daf.fairly_active_mins, 
    daf.lightly_active_mins, 
    daf.sedentary_mins,
    dsf.total_mins_asleep,
    dsf.total_mins_in_bed
FROM daily_activity_formatted daf
LEFT JOIN
daily_sleep_formatted dsf
USING (id)
;

```


I reformatted the weight table to be consisted with the rest of the changes I have made. 
```{sql format and filter weight_log table, connection=bellabeat_con}
-- Remove fat and weight in pounds columns because they are not necessary for the analysis. Also, the fat column is missing a lot of data
CREATE TABLE  IF NOT EXISTS weight AS
SELECT 
	id, 
    date(date) AS date, 
    weightKg as weight_kg,
    BMI as bmi
FROM 
weight_log
WHERE id <> "4057192912"
```
```{sql connection=bellabeat_con}
SELECT * FROM weight;

```





#### 4) Anlayze

The "Analyze" phase involves getting the data in the final form and analyzing it. For the analysis steps I created new metrics by averaging values in the table and joined various tables together.


First, I combined the hourly calories and hourly steps data and averaged them across dates. This table is for spotting trends over the course of a day.
```{sql combine hourly calories and hourly steps, connection=bellabeat_con}
-- Combine calories and steps to one table create averages for hours across all days
-- REMOVE 2016-05-12 AND 4057192912 
CREATE TABLE IF NOT EXISTS hourly_avg_steps_calories AS 
SELECT hc.id, hc.time, hc.avg_calories, hs.avg_steps 
FROM 
	(SELECT id, time, AVG(calories) AS avg_calories
	FROM
		hourly_calories_formatted 
	WHERE id <> "4057192912" AND date <> "2016-05-12"
	GROUP BY id, time
    ) hc
LEFT JOIN 
	(
    SELECT id, time, AVG(steptotal) AS avg_steps
	FROM
		hourly_steps_formatted 
	WHERE id <> "4057192912" AND date <> "2016-05-12"
	GROUP BY id, time) hs
USING (id, time)
UNION
SELECT hc.id, hc.time, hc.avg_calories, hs.avg_steps 
FROM 
	(SELECT id, time, AVG(calories) AS avg_calories
	FROM
		hourly_calories_formatted 
	WHERE id <> "4057192912" AND date <> "2016-05-12"
	GROUP BY id, time
    ) hc
RIGHT JOIN 
	(
    SELECT id, time, AVG(steptotal) AS avg_steps
	FROM
		hourly_steps_formatted 
	WHERE id <> "4057192912" AND date <> "2016-05-12"
	GROUP BY id, time) hs
USING (id, time)
;
```
```{sql hourly_avg_steps_calories, connection=bellabeat_con}
SELECT * FROM hourly_avg_steps_calories
```



I decided to assign an "activeness level" to the participants. Grouping the participants by activeness level makes it easier to spot trends. The participant were assigned "VERY_ACTIVE", "ACTIVE", "NOT_ACTIVE" based on their average active minutes data.
```{sql activeness level assigned to each participant, connection=bellabeat_con}
CREATE TABLE IF NOT EXISTS participant_avg_daily_activity_and_sleep AS 
SELECT 
	id, 
    AVG(total_steps) AS avg_total_steps, 
    AVG(total_calories) AS avg_total_calories,
	AVG(very_active_mins) AS avg_very_active_mins, 
	AVG(fairly_active_mins) AS avg_fairly_active_mins,
    AVG(lightly_active_mins) AS avg_lighly_active_mins,
    AVG(sedentary_mins) AS avg_sedentary_mins,
    AVG(total_mins_asleep) AS avg_total_mins_asleep,
    AVG(total_mins_in_bed) AS total_mins_in_bed,
	CASE
    WHEN AVG(very_active_mins) > 30 THEN 'VERY_ACTIVE'
    WHEN AVG(fairly_active_mins) >20 OR AVG(lightly_active_mins) >120 THEN 'ACTIVE'
    ELSE 'NOT_ACTIVE'
    END AS activeness_level
FROM daily_activity_and_sleep
GROUP BY id
;

```
```{sql participant_avg_daily_activity_and_sleep, connection=bellabeat_con}
SELECT * FROM participant_avg_daily_activity_and_sleep;
```



I created a new table to study overall trends across participants. This is to determine if an average consumer exhibits the same behavior day to day.
```{sql avg pariticpants to get daily values, connection=bellabeat_con}

CREATE TABLE IF NOT EXISTS daily_avg_activity_and_sleep AS 
SELECT 
	date, 
    AVG(total_steps) AS avg_total_steps, 
    AVG(total_calories) AS avg_total_calories,
	AVG(very_active_mins) AS avg_very_active_mins, 
	AVG(fairly_active_mins) AS avg_fairly_active_mins,
    AVG(lightly_active_mins) AS avg_lighly_active_mins,
    AVG(sedentary_mins) AS avg_sedentary_mins,
    AVG(total_mins_asleep) AS avg_total_mins_asleep,
    AVG(total_mins_in_bed) AS total_mins_in_bed
FROM daily_activity_and_sleep
GROUP BY date
;
```
```{sql daily_avg_activity_and_sleep, connection=bellabeat_con}
SELECT * FROM daily_avg_activity_and_sleep;
```

Finally, I export the data tables to R for the next phase.
```{sql export daily_avg_activity_and_sleep, connection=bellabeat_con, output.var = "daily_avg_activity_and_sleep"}
-- output table
SELECT * FROM daily_avg_activity_and_sleep
```
```{sql export participant_avg_daily_activity_and_sleep, connection=bellabeat_con, output.var = "participant_avg_daily_activity_and_sleep"}
-- output table
SELECT * FROM participant_avg_daily_activity_and_sleep
```
```{sql export hourly_avg_steps_calories, connection=bellabeat_con, output.var = "hourly_avg_steps_calories"}
-- output table
SELECT * FROM hourly_avg_steps_calories
```
```{sql export weight, connection=bellabeat_con, output.var = "weight"}
-- output table
SELECT * FROM weight
```


#### 5) Share

The "Share" phase includes creating visuals to show the data analyses. 

I used both R and Tableau to create visuals. The Tableau visuals can be found [here](https://public.tableau.com/app/profile/petek2584/viz/CaseStudyBellabeat/Dashboard1)

First, lets look at daily data.
```{r daily steps}
ggplot(data= daily_avg_activity_and_sleep, aes(x=date,y= avg_total_steps, group = 1)) +geom_line(color="blue")+geom_point(color="blue")+  theme(axis.text.x = element_text( angle=45))+
   ylim(0, 10000) +
  labs(title="Average Steps Walked Each Day", y="Avg. Steps",
       x="Date")


```
This plot shows that there is no trend between days. The number of steps walked is sporadic between days and there is no specific date that stands out. This is expected. I wanted to make sure there were not outlier days that would influence the average.


Now, we can look at averaged hourly data.
```{r avg steps and avg calories within 24 hours}
avg_by_time <- hourly_avg_steps_calories %>%
  group_by(time) %>%
  drop_na() %>%
  summarise(avg_step = mean(avg_steps), avg_calorie = mean(avg_calories))

ggplot(data= avg_by_time, aes(x=time, group = 1)) + 
  geom_line(aes(y = avg_step, color = "Steps")) + geom_point(aes(y = avg_step, color = "Steps"))+
  geom_line(aes(y = avg_calorie/0.2, color="Calories")) +geom_point(aes(y = avg_calorie/0.2, color="Calories"))+
  scale_y_continuous(
    # Features of the first axis
    name = "Avg. Steps Walked",
    # Add a second axis and specify its features
    sec.axis = sec_axis(~ . * 0.2, name="Avg. Calories Burned")
  )+
  scale_color_manual( values = c("Steps" = "pink", "Calories" = "steelblue"))+
  theme(axis.text.x = element_text( angle=45))+
  labs(title="Average Steps Walked and Average Calroies Burned Within 24 hr Period", 
       x="Time (hr)")


```

The plot shows us a positive correlation between calories burned and number of steps walked. This means that the participants are burning more calories in the hours they are walking more steps. This is to be expected. The more active the participants are the more calories they are burning. What is more interesting about this plot is which hours in the day the participants walking the most number of steps. Next, we can take a close look at the hourly step data.

```{r avg steps whitin 24 horus}
ggplot(data= avg_by_time, aes(x=time, y=avg_step)) + geom_bar(stat='identity', fill="pink")+
  theme(axis.text.x = element_text( angle=45))+
  labs(title="Average Steps Walked in 24 hours",y="Avg. Steps",
       x="Time")
```
The hourly step data shows us what times during the day the participants are most active. As expected, midnight to 5:00 AM includes the minimum number of steps walked. There are two separate peaks in steps, one peak around noon and another peak around 18:00 (6:00 PM). This can potentially mean that people are more active during their lunch break and after business hours. Keeping this in mind, Bellabeat can be marketed as a smart device to encourage and keep track of post-work workout.


Next, we can look at the sleep data between different activeness level. I wanted to see if there is a correlation between activeness level and sleep habbits.
```{r minutes sleeping by activeness level}
avg_by_participant <- participant_avg_daily_activity_and_sleep %>%
  group_by(activeness_level) %>%
  drop_na() %>%
  summarise(asleep_p = (mean(avg_total_mins_asleep)/mean(total_mins_in_bed))*100)


ggplot(data= avg_by_participant, aes(x=activeness_level, y=asleep_p)) + geom_bar(stat='identity', fill="pink")+
  ylim(0, 100) +   geom_text(aes(label = round(asleep_p) ), vjust = -0.2) + labs(title="Percent Time in Bed Spent Sleeping",y="Percent Minutes Sleeping",
       x="Activeness Level")


```
Looking at the plot, there does not seem to be a correlation between activeness level and sleep habit. The "very active" participant spend 95% of their time in bed sleeping, while "active" participants spend 90% and "not active" participant spend 85%. There is a trend of where the more active you are the more time you spend sleeping in bed. However, it is important to note that the difference in percentage is very small and this slight correlation does not mean causation.

Next, we can look at hourly step data again but group the data by activeness level.
```{r avg steps within 24 hour by activeness level}
hourly_data_by_activeness_level <- hourly_avg_steps_calories %>% 
  left_join(participant_avg_daily_activity_and_sleep, by=c('id'='Id')) %>% 
  group_by(time) %>%
  drop_na()%>%
  summarise(very_active = mean(avg_steps[activeness_level=="VERY_ACTIVE"]),active = mean(avg_steps[activeness_level=="ACTIVE"]),
            not_active = mean(avg_steps[activeness_level=="NOT_ACTIVE"]))


ggplot(data= hourly_data_by_activeness_level, aes(x=time, group = 1)) + 
  geom_line(aes(y = very_active, color="very_active")) + geom_point(aes(y = very_active, color="very_active"))+
  geom_line(aes(y = active, color="active")) +geom_point(aes(y = active, color="active"))+
  geom_line(aes(y = not_active, color="not_active")) +geom_point(aes(y = not_active, color="not_active"))+
    scale_color_manual( values = c("very_active"= "green", "active" = "purple", "not_active" = "steelblue"))+
    theme(axis.text.x = element_text( angle=45))+
    labs(title="Average Steps Walked By Activeness Level",y="Avg. Number of Steps",
         x="Time")

```


This plot is more interesting and gives us insights about how groups of different activeness level behave. We can see from the plot that "not active" people are taking the same number of steps per hour throughout the day. They are move active between 8:00 to 22:00 but that is to be expected. They are walking more during the day compared to at night.
The "active" group has greater difference in number of steps between day and night. The "active" group also has a noticeable peak in number of steps around 18:00. This peak might indicate the time when the "active" group chooses to exercise. Considering the time is 18:00, this most likely meanz the "active" group prefers to workout right after business hours. 
Lastly, the "very active" group has an interesting pattern with three separate peaks in number of steps. The "very active" group exercises at
  1) in the morning around 7:00. This is most likely before work day begins.
  2) around noon most likely around lunch break.
  3) in the afternoon around 18:00. This is most likely after business hours so after they finish work or school.

#### 6) Act

The "Act" phase is for making data-driven decision based on the data analysis.

After formatting, cleaning, analyzing and plotting the data I observed several trends that can inform Bellabeat's marketing strategy. 

* The data shows an expected positive correlation between calories burned and number of steps walked. In this aspect, Bellabeat can be marketted to individuals who are looking to burn more calories. Bellabeat can keep track of their steps and provide feedback on number of calories they have burned. This can help the consumer reach their weight loss goals. More data on participant's weight is required to make more accurate recommendations
* The data does not show strong trends between activeness level and percentage of time in bed spent sleeping. 
* Moderately active and very active individuals have different patterns in exercise. Moderately active participants choose to workout around 18:00. While very active particpants work out at three different time points during the day: 7:00, 12:00, and 18:00. To reach the most number of consumers, it would be better to target the 18:00 block. The Bellabeat smart device can be marketed as a way to encourage people to get their post-work exercise in. If Bellabeat wants to target more active consumers, then Bellabeat can be marketed for early-birds who prefer to get up early can exercise before the day begins. 


While these recommendation can help Bellabeat's marketing strategy, it is also important to note the assumptions made in this analysis and the limitations of the data. These include:

* Assumed there were 33 participants instead of 30 as the data source stated.

* The data was collected from participants during their routine daily activities.

* The data is not current. It is 6 years old.

* The sample size of 33 participants is very small. The 33 participants may not be an unbiased representation of the whole population. 

* The data is not complete or consistent. The data includes many blank cells, and not all participants have the same number of data points.

* The data is only on one device, Fitbit. More data with multiple different smart devices is necessary to understand overall trends with smart devices.










